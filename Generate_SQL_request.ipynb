{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aichaoukdour/SQL-To-text-Gemma-3-27B/blob/main/Generate_SQL_request.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "We13eYWi_QZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f4029f-78e7-45b4-85ec-52f0d0ef18df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "420GBT0DUBU-"
      },
      "source": [
        "# **Importation des modules nécessaires :**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0LKMuNAkabPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd677d9-c5a2-4935-e224-0a5ea2a80b57",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence_transformers\n",
        "import json\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKk7KizRal5n"
      },
      "source": [
        "**l'API Gemini**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6GO0Qv7aar5R"
      },
      "outputs": [],
      "source": [
        "# 3. Configure Gemini API\n",
        "genai.configure(api_key=\"AIzaSyBdLqxG6_hcuer7C2IVRrbfxdYTUa20nSc\")\n",
        "model = genai.GenerativeModel(model_name=\"gemma-3-27b-it\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3naa6Fy3av2g"
      },
      "source": [
        "**Chargement des métadonnée**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PUHQOsGNa1rP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bddd2c-e350-46b5-fba2-595932236761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fichier metadata_organisee.json généré avec succès.\n"
          ]
        }
      ],
      "source": [
        "# Charger les métadonnées depuis le fichier\n",
        "with open(\"metadata1.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# Réorganiser les données\n",
        "organized_metadata = []\n",
        "\n",
        "for table_name, content in metadata.items():\n",
        "    organized_metadata.append({\n",
        "        \"table_name\": table_name,\n",
        "        \"description\": content.get(\"description_fr\"),\n",
        "        \"schema\": content.get(\"schema\", \"Pas de schéma\")\n",
        "    })\n",
        "\n",
        "# Sauvegarder dans un nouveau fichier JSON bien structuré\n",
        "with open(\"metadata_organisee.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(organized_metadata, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"✅ Fichier metadata_organisee.json généré avec succès.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzywuzzy\n",
        "!pip install rank_bm25\n",
        "!pip install nltk"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZdbNPl78pwN",
        "outputId": "c3f5de29-3a01-486a-8e88-1189c09e6d3a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "from functools import lru_cache\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Download NLTK resources\n",
        "def download_nltk_resources():\n",
        "    for resource in ['punkt', 'stopwords']:\n",
        "        try:\n",
        "            nltk.data.find(f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}')\n",
        "            logger.info(f\"Resource {resource} already downloaded\")\n",
        "        except LookupError:\n",
        "            logger.info(f\"Downloading {resource}...\")\n",
        "            nltk.download(resource)\n",
        "\n",
        "# Initialize stopwords\n",
        "download_nltk_resources()\n",
        "STOPWORDS = set(stopwords.words('french'))\n",
        "\n",
        "class TableExtractor:\n",
        "    def __init__(self, metadata_path=\"metadata_organisee.json\"):\n",
        "        \"\"\"Initialize the table extractor with metadata path\"\"\"\n",
        "        self.metadata_path = metadata_path\n",
        "        self.metadata = self._load_metadata()\n",
        "\n",
        "        # Initialize tokenizer and model from Hugging Face\n",
        "        logger.info(\"Loading language model...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
        "        self.model = AutoModel.from_pretrained(\"camembert-base\")\n",
        "\n",
        "        # Index metadata\n",
        "        logger.info(\"Indexing metadata...\")\n",
        "        self._prepare_indices()\n",
        "\n",
        "        # Cache for embeddings to improve performance\n",
        "        self.embedding_cache = {}\n",
        "\n",
        "    def _load_metadata(self):\n",
        "        \"\"\"Load metadata from JSON file\"\"\"\n",
        "        try:\n",
        "            with open(self.metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                metadata = json.load(f)\n",
        "\n",
        "            # Check if metadata is a list\n",
        "            if not isinstance(metadata, list):\n",
        "                logger.info(\"Converting metadata to list format...\")\n",
        "                if \"tables\" in metadata:\n",
        "                    metadata = metadata[\"tables\"]\n",
        "                else:\n",
        "                    metadata = [metadata]\n",
        "\n",
        "            logger.info(f\"Loaded {len(metadata)} tables from metadata\")\n",
        "            return metadata\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading metadata: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _prepare_indices(self):\n",
        "        \"\"\"Prepare various indices for table search\"\"\"\n",
        "        # Extract table names and full text for each table\n",
        "        self.table_names = []\n",
        "        self.table_descriptions = []\n",
        "        self.table_full_texts = []\n",
        "        self.table_keywords = []\n",
        "\n",
        "        # Process each metadata entry\n",
        "        for entry in self.metadata:\n",
        "            table_name = entry.get('table_name', '')\n",
        "            description = entry.get('description', '')\n",
        "            schema = entry.get('schema', '')\n",
        "\n",
        "            # Skip entries without table name\n",
        "            if not table_name:\n",
        "                continue\n",
        "\n",
        "            # Generate full text representation\n",
        "            full_text = f\"Table: {table_name}. Description: {description}. Schema: {schema}.\"\n",
        "\n",
        "            # Extract keywords from table name and description\n",
        "            table_parts = re.split(r'[_\\s]', table_name)\n",
        "            keywords = [part.lower() for part in table_parts if part.lower() not in STOPWORDS and len(part) > 2]\n",
        "\n",
        "            # Add to our lists\n",
        "            self.table_names.append(table_name)\n",
        "            self.table_descriptions.append(description)\n",
        "            self.table_full_texts.append(full_text)\n",
        "            self.table_keywords.append(keywords)\n",
        "\n",
        "        # Create BM25 index\n",
        "        logger.info(\"Creating BM25 index...\")\n",
        "        tokenized_corpus = [word_tokenize(text.lower()) for text in self.table_full_texts]\n",
        "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "        # Generate embeddings for table names and descriptions\n",
        "        logger.info(\"Generating embeddings for tables...\")\n",
        "        self.table_embeddings = self._get_embeddings(self.table_full_texts)\n",
        "\n",
        "    @lru_cache(maxsize=100)\n",
        "    def _get_embedding(self, text):\n",
        "        \"\"\"Get embedding for a single text\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Use mean of last hidden state as embedding\n",
        "        return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    def _get_embeddings(self, texts):\n",
        "        \"\"\"Get embeddings for a list of texts\"\"\"\n",
        "        embeddings = []\n",
        "        batch_size = 8  # Process in small batches to avoid memory issues\n",
        "\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            batch_embeddings = [self._get_embedding(text) for text in batch_texts]\n",
        "            embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return np.array(embeddings)\n",
        "\n",
        "    def _extract_query_info(self, query):\n",
        "        \"\"\"Extract relevant information from the query\"\"\"\n",
        "        # Clean query\n",
        "        query = query.strip()\n",
        "\n",
        "        # Extract potential table name patterns\n",
        "        # Pattern 1: words with underscore - common in table names\n",
        "        underscored = re.findall(r'\\b([A-Za-z]+_[A-Za-z0-9_]+)\\b', query)\n",
        "\n",
        "        # Pattern 2: capitalized expressions\n",
        "        capitalized = re.findall(r'\\b([A-Z][a-zA-Z0-9]*(?:[_][a-zA-Z0-9]+)*)\\b', query)\n",
        "\n",
        "        # Pattern 3: Text after keywords like \"table\", \"extraction\", \"données\"\n",
        "        indicators = re.findall(r'(?:table|tableau|extraction|données|fichier)s?\\s+(?:de|du|des)?\\s+(?:la|les|le)?\\s+([a-zA-Z0-9_]+)', query.lower())\n",
        "\n",
        "        # Combine all patterns\n",
        "        potential_tables = list(set(underscored + capitalized + indicators))\n",
        "\n",
        "        # Tokenize for BM25\n",
        "        tokens = word_tokenize(query.lower())\n",
        "        filtered_tokens = [token for token in tokens if token not in STOPWORDS and len(token) > 2]\n",
        "\n",
        "        return {\n",
        "            'query': query,\n",
        "            'tokens': filtered_tokens,\n",
        "            'potential_tables': potential_tables\n",
        "        }\n",
        "\n",
        "    def extract_relevant_tables(self, query, top_k=20):\n",
        "        \"\"\"Extract the most relevant tables based on the query\"\"\"\n",
        "        start_time = time.time()\n",
        "        query_info = self._extract_query_info(query)\n",
        "\n",
        "        # Get query embedding\n",
        "        query_embedding = self._get_embedding(query_info['query'])\n",
        "\n",
        "        # Calculate semantic similarity\n",
        "        semantic_scores = cosine_similarity([query_embedding], self.table_embeddings)[0]\n",
        "\n",
        "        # Calculate BM25 scores\n",
        "        bm25_scores = self.bm25.get_scores(query_info['tokens'])\n",
        "\n",
        "        # Normalize scores\n",
        "        if np.max(semantic_scores) > 0:\n",
        "            semantic_scores = semantic_scores / np.max(semantic_scores)\n",
        "        if np.max(bm25_scores) > 0:\n",
        "            bm25_scores = bm25_scores / np.max(bm25_scores)\n",
        "\n",
        "        # Calculate exact match scores\n",
        "        exact_match_scores = np.zeros(len(self.table_names))\n",
        "\n",
        "        for i, table_name in enumerate(self.table_names):\n",
        "            # Direct table name match\n",
        "            for potential in query_info['potential_tables']:\n",
        "                if potential.lower() == table_name.lower():\n",
        "                    exact_match_scores[i] += 1.0\n",
        "                elif potential.lower() in table_name.lower():\n",
        "                    exact_match_scores[i] += 0.5\n",
        "                elif table_name.lower() in potential.lower():\n",
        "                    exact_match_scores[i] += 0.3\n",
        "\n",
        "            # Keyword match\n",
        "            common_keywords = set(self.table_keywords[i]).intersection(set(query_info['tokens']))\n",
        "            exact_match_scores[i] += len(common_keywords) * 0.2\n",
        "\n",
        "        # Normalize exact match scores\n",
        "        if np.max(exact_match_scores) > 0:\n",
        "            exact_match_scores = exact_match_scores / np.max(exact_match_scores)\n",
        "\n",
        "        # Combine scores with weights\n",
        "        combined_scores = 0.5 * semantic_scores + 0.3 * bm25_scores + 0.2 * exact_match_scores\n",
        "\n",
        "        # Get top K indices\n",
        "        top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
        "\n",
        "        # Prepare results\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            table_name = self.table_names[idx]\n",
        "            entry = next((item for item in self.metadata if item.get('table_name') == table_name), {})\n",
        "\n",
        "            results.append({\n",
        "                'table': table_name,\n",
        "                'score': float(combined_scores[idx]),\n",
        "                'description': entry.get('description', ''),\n",
        "                'schema': entry.get('schema', ''),\n",
        "                'details': {\n",
        "                    'semantic_score': float(semantic_scores[idx]),\n",
        "                    'bm25_score': float(bm25_scores[idx]),\n",
        "                    'exact_match_score': float(exact_match_scores[idx])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        logger.info(f\"Query processed in {elapsed:.3f} seconds\")\n",
        "\n",
        "        return results\n",
        "\n",
        "# Main function to demonstrate usage\n",
        "def main():\n",
        "    # Initialize extractor\n",
        "    table_extractor = TableExtractor()\n",
        "\n",
        "    # Example queries\n",
        "    test_queries = [\" e vous prie de nous fournir l'extraction MDN_par_PROFIL des clients iDar pour le Mois de mars  2025.\"]\n",
        "    # Process each query\n",
        "    all_results = {}\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nProcessing query: {query}\")\n",
        "        results = table_extractor.extract_relevant_tables(query)\n",
        "        all_results[query] = results\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\nTop 5 most relevant tables:\")\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"{i+1}. {result['table']} (Score: {result['score']:.4f})\")\n",
        "            print(f\"   Details: Semantic={result['details']['semantic_score']:.2f}, \"\n",
        "                  f\"BM25={result['details']['bm25_score']:.2f}, \"\n",
        "                  f\"Exact Match={result['details']['exact_match_score']:.2f}\")\n",
        "\n",
        "    # Save results\n",
        "    with open('relevant_tables_results.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "    print(\"\\nResults saved to 'relevant_tables_results.json'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I243xgmaRSV-",
        "outputId": "774a68b5-675d-473a-f5e1-198d7f3a5d5b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing query:  e vous prie de nous fournir l'extraction MDN_par_PROFIL des clients iDar pour le Mois de mars  2025.\n",
            "\n",
            "Top 5 most relevant tables:\n",
            "1. dn_ln_events_d1 (Score: 0.7112)\n",
            "   Details: Semantic=0.91, BM25=0.86, Exact Match=0.00\n",
            "2. dn_parc_post_d1 (Score: 0.7000)\n",
            "   Details: Semantic=0.80, BM25=1.00, Exact Match=0.00\n",
            "3. post_dn_arpu_m1 (Score: 0.6364)\n",
            "   Details: Semantic=0.95, BM25=0.53, Exact Match=0.00\n",
            "4. post_arpu_invoice_m1 (Score: 0.5954)\n",
            "   Details: Semantic=0.89, BM25=0.51, Exact Match=0.00\n",
            "5. dn_arpu_detail_data_m1 (Score: 0.5943)\n",
            "   Details: Semantic=0.88, BM25=0.52, Exact Match=0.00\n",
            "6. dn_arpu_detail_sms_m1 (Score: 0.5904)\n",
            "   Details: Semantic=0.89, BM25=0.49, Exact Match=0.00\n",
            "7. dn_arpu_detail_voice_m1 (Score: 0.5828)\n",
            "   Details: Semantic=0.88, BM25=0.48, Exact Match=0.00\n",
            "8. dn_ln_prep_inter_sap_exp (Score: 0.5255)\n",
            "   Details: Semantic=1.00, BM25=0.09, Exact Match=0.00\n",
            "9. dn_ln_cpe_d1 (Score: 0.5235)\n",
            "   Details: Semantic=1.00, BM25=0.08, Exact Match=0.00\n",
            "10. post_arpu_clb_d1 (Score: 0.5100)\n",
            "   Details: Semantic=0.97, BM25=0.08, Exact Match=0.00\n",
            "11. dn_ln_cpr_d1 (Score: 0.5097)\n",
            "   Details: Semantic=0.97, BM25=0.08, Exact Match=0.00\n",
            "12. customer_d1 (Score: 0.5077)\n",
            "   Details: Semantic=0.97, BM25=0.08, Exact Match=0.00\n",
            "13. seg_rch_kpi_d1 (Score: 0.5063)\n",
            "   Details: Semantic=0.95, BM25=0.11, Exact Match=0.00\n",
            "14. customer_m1 (Score: 0.5052)\n",
            "   Details: Semantic=0.96, BM25=0.08, Exact Match=0.00\n",
            "15. post_arpu_recharge_m1 (Score: 0.4973)\n",
            "   Details: Semantic=0.94, BM25=0.09, Exact Match=0.00\n",
            "16. ref_region_province (Score: 0.4972)\n",
            "   Details: Semantic=0.95, BM25=0.08, Exact Match=0.00\n",
            "17. prep_arpu_clb_d1 (Score: 0.4969)\n",
            "   Details: Semantic=0.99, BM25=0.00, Exact Match=0.00\n",
            "18. post_arpu_interco_d1 (Score: 0.4950)\n",
            "   Details: Semantic=0.95, BM25=0.07, Exact Match=0.00\n",
            "19. dn_clr_blnce_kpi_d1 (Score: 0.4927)\n",
            "   Details: Semantic=0.94, BM25=0.07, Exact Match=0.00\n",
            "20. dn_ln_ident_d1 (Score: 0.4917)\n",
            "   Details: Semantic=0.93, BM25=0.08, Exact Match=0.00\n",
            "\n",
            "Results saved to 'relevant_tables_results.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "vsRIzAu4bGNL",
        "outputId": "b6833783-8992-4ab2-8c26-2b49767f9401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "You are an expert-level SQL query generator with deep understanding of the Telecommunications (Telecom) domain and common analytics requests within this industry.\n",
            "\n",
            "# TASK\n",
            "\n",
            "Your primary task is to translate a user's natural language request into an accurate, efficient, and syntactically correct SQL query, based on the provided database schema.\n",
            "{\n",
            "  \" e vous prie de nous fournir l'extraction MDN_par_PROFIL des clients iDar pour le Mois de mars  2025.\": [\n",
            "    {\n",
            "      \"table\": \"dn_ln_events_d1\",\n",
            "      \"score\": 0.7112470031365974,\n",
            "      \"description\": \" Suit les \\u00e9v\\u00e9nements quotidiens des clients, couvrant diff\\u00e9rentes p\\u00e9riodes (1 semaine, 1 mois, 3 mois) et les \\u00e9v\\u00e9nements vocaux.\",\n",
            "      \"schema\": \"CREATE TABLE dn_ln_events_d1 (\\n  mdn string,\\n  nb_events_in_off_1w bigint,\\n  nb_events_in_off_1m bigint,\\n  nb_events_in_off_3m bigint,\\n  nb_events_in_voice_1m bigint,\\n  ingestion_date timestamp\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9063767790794373,\n",
            "        \"bm25_score\": 0.8601953786562625,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_parc_post_d1\",\n",
            "      \"score\": 0.7000267982482911,\n",
            "      \"description\": \" G\\u00e8re les abonn\\u00e9s post-pay\\u00e9s avec des d\\u00e9tails sur l\\u2019activation, la migration, les motifs d\\u2019annulation, la date de r\\u00e9engagement, les d\\u00e9tails SIM et l\\u2019\\u00e9tat du r\\u00e9seau, l\\u2019IMSI et l\\u2019IMEI.\",\n",
            "      \"schema\": \"CREATE TABLE dn_parc_post_d1 (\\n    start_date timestamp -- date de cr\\u00e9ation du compte,\\n    customer_id string -- id compte,\\n    subscriber string -- id compte,\\n    client_id string -- id client,\\n    subscription_id string -- id souscription,\\n    contract_code string -- code du contract,\\n    subscription_id_mig_old string -- id souscription PS,\\n    flag_mig_profil string -- client migr\\u00e9 ou non, valeurs possibles (No, Yes),\\n    sf_subscription_id string -- id souscription SF,\\n    mdn string -- num\\u00e9ro de t\\u00e9l\\u00e9phone du client,\\n    activation_bscs_date timestamp -- date activation du compte de facturation,\\n    installed_date_crm string -- date installation du compte crm,\\n    effective_date timestamp -- date activation du compte de facturation,\\n    v_cancellation_date timestamp -- date annulation du contract,\\n    v_expiration_date timestamp -- date expiration du contract,\\n    suspension_date timestamp -- date suspenssion du contract,\\n    crm_status string -- status du compte, valeurs possibles (EXPIRED, Suspension Facturation, InwiB2C_Expired, active, InwiB2C_Active),\\n    offer_code string -- code de l'offre, valeurs possibles (OSIMPOSTB2C, OPAPOSTB2C),\\n    crm_profil string -- id de l'offre, valeurs possibles (86916, 81385),\\n    segment string -- B2C/B2B, valeurs possibles (B2C, B2B),\\n    formule string -- Pospaid/Prepaid, valeurs possibles (Postpaid, Prepaid),\\n    desc_profil string -- decription de l'offre, valeurs possibles (Idar Duo 249 dhs, Forfait 49dhs Internet, 18GO+5h+1hinter+illimit\\u00e9 wapp, Idar Duo 199 dhs, Forfait 49dhs illimite RS),\\n    produit string -- decription du produit, valeurs possibles (Idar Duo 249 dhs, Forfait 49dhs Internet, 18GO+5h+1hinter+illimit\\u00e9 wapp, Idar Duo 199 dhs, Forfait 49dhs illimite RS),\\n    gamme string -- type d'abonnement, valeurs possibles (ADSL, Autres forfaits, Forfaits 49 dhs, Forfaits Hors 99 dhs, Idar, DataOnly Postpaid, Forfaits 99 dhs, FTTH),\\n    marche string -- Home/Mobile, valeurs possibles (Home, Mobile Postpaid),\\n    typ_ligne string -- type de ligne,\\n    sim_technology string -- type du SIM 2G/3G/4G, valeurs possibles (4G, 2G, GSM),\\n    article_code_sim string -- code du type SIM,\\n    article_code_terminal string -- code du mat\\u00e9riel,\\n    imsi string -- code imsi unique du client,\\n    imei_sim string -- code sim unique du client,\\n    imei_terminal string -- code du materiel,\\n    desc_terminal string -- description du materiel, valeurs possibles (MODEM BOX 4G HUAWEI B311-221),\\n    activation_sales_channel string -- code du distributeur d'activation,\\n    activation_pos string -- canal de vente,\\n    engagement_duration int -- nombre de mois d'engagement,\\n    region string -- region,\\n    supervisor string -- superviseur commerciale,\\n    prestataire string -- distributeur commerciale,\\n    distributeur string -- distributeur commerciale,\\n    segment_activation_channel string -- canal d'activation, valeurs possibles (Boutique, VI, VS),\\n    porta_flag string -- flag du client s'il a fait une portabilit\\u00e9,\\n    porta_date timestamp -- date de portabilit\\u00e9,\\n    porta_operator string -- l'op\\u00e9rateur du portabilit\\u00e9, valeurs possibles (ORANGE, MAROCTELECOM),\\n    migration_flag string -- flag du compte s'il a fait une migration,\\n    migration_date string -- date de migration,\\n    acquisition_type string -- nouveau compte ou compte migr\\u00e9, valeurs possibles (migration, portability),\\n    recycling_status string -- status de recyclage,\\n    dunning_status string -- staus de suspension, valeurs possibles (Suspendue facturation, Restriction Recouvrement, Suspendu facturation client, Suspension Recouvrement),\\n    cancellation_reason string -- code de raison d'annulation,\\n    cancellation_reason_detail string -- d\\u00e9tail de raison d'annulation,\\n    cancellation_initiator string -- l'initiateur d'annulation, valeurs possibles (OPERATEUR, CLIENT),\\n    engagement_status string -- status d'engagement,\\n    first_reengagement_date timestamp --  date de premier r\\u00e9engagement,\\n    first_reengagement_duration int --  dur\\u00e9e de premier r\\u00e9engagement,\\n    last_reengagement_date timestamp -- date de dernier r\\u00e9engagement,\\n    last_reengagement_duration int -- dur\\u00e9e de d\\u00e9rnier r\\u00e9engagement,\\n    top_city string -- la ville d'activation du compte,\\n    top_region string -- la r\\u00e9gion d'activation du compte,\\n    top_bts string -- le code radar d'activation du compte,\\n    top_province string -- la province d'activation du compte,\\n    crm_profil_actual string -- id de l'offre actuelle,\\n    mdn_contact string -- num\\u00e9ro de t\\u00e9l\\u00e9phone de l'agent,\\n    flag_terminal string -- si le client a pris le materiel ou non,\\n    statut_ligne string -- le statut de la ligne, valeurs possibles (actif, resile),\\n    satisfait_remb string -- si le client est satisfait ou rembours\\u00e9,\\n    ingestion_date timestamp -- la date d'ingestion des donn\\u00e9es,\\n    id_date string -- la date de la partition,\\n    id_date_ex string -- la date de la partition format\\u00e9\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.800053596496582,\n",
            "        \"bm25_score\": 1.0,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"post_dn_arpu_m1\",\n",
            "      \"score\": 0.6364271931333378,\n",
            "      \"description\": \" Stocke des informations sur les revenus g\\u00e9n\\u00e9r\\u00e9s par chaque client ou abonnement, aidant \\u00e0 calculer le revenu moyen par utilisateur (ARPU). Elle montre les d\\u00e9tails des clients, les services utilis\\u00e9s, le type de revenu (appels/donn\\u00e9es), et le revenu total g\\u00e9n\\u00e9r\\u00e9 pendant chaque cycle de facturation (mois, ann\\u00e9e, etc.).\",\n",
            "      \"schema\": \"CREATE TABLE post_dn_arpu_m1 (\\n  bill_cycle timestamp,\\n  account_id string,\\n  customer_code string,\\n  subscription_id string,\\n  dn string,\\n  profile string,\\n  subprofile string,\\n  produit string,\\n  gamme string,\\n  marche string,\\n  revenue_type string,\\n  traffic_direction string,\\n  revenue_type_group string,\\n  arpu_amount double,\\n  arpu_amount_ht double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9531092643737793,\n",
            "        \"bm25_score\": 0.5329085364881606,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"post_arpu_invoice_m1\",\n",
            "      \"score\": 0.5954465587896698,\n",
            "      \"description\": \" Suivi des revenus des frais d\\u2019activation des services postpay\\u00e9s au premier et au dix-huiti\\u00e8me jour de chaque mois, avec des d\\u00e9tails sur la facture et le montant factur\\u00e9.\",\n",
            "      \"schema\": \"CREATE TABLE post_arpu_invoice_m1 (\\n  bill_cycle timestamp,\\n  customer_id string,\\n  customer_code string,\\n  account_id string,\\n  subscription_id string,\\n  dn string,\\n  rateplan_code string,\\n  profile_id string,\\n  subprofile string,\\n  produit string,\\n  gamme string,\\n  marche string,\\n  revenue_type string,\\n  revenue_type_group string,\\n  invoice_reference string,\\n  billed_amount double,\\n  currency string,\\n  segment string\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.8873275518417358,\n",
            "        \"bm25_score\": 0.5059426095626728,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_arpu_detail_data_m1\",\n",
            "      \"score\": 0.5943248945053393,\n",
            "      \"description\": \" Contient des informations d\\u00e9taill\\u00e9es sur les revenus et les soldes de chaque client ou compte par mois.\",\n",
            "      \"schema\": \"CREATE TABLE dn_arpu_detail_data_m1 (\\n  start_date timestamp,\\n  end_date timestamp,\\n  dn string,\\n  contract_id string,\\n  party_customer string,\\n  party_account string,\\n  product_id string,\\n  subprofile string,\\n  product_name string,\\n  segment string,\\n  gamme string,\\n  marche string,\\n  billing_type string,\\n  balance_id string,\\n  balance_description string,\\n  balance_revenue_flag string,\\n  balance_unit string,\\n  balance_amount bigint,\\n  revenue_amount double,\\n  revenue_amount_ht double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.8758257031440735,\n",
            "        \"bm25_score\": 0.5213734764443421,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_arpu_detail_sms_m1\",\n",
            "      \"score\": 0.5903724618063744,\n",
            "      \"description\": \" Contient des informations d\\u00e9taill\\u00e9es sur les revenus des SMS et les soldes pour chaque client ou compte par mois.\",\n",
            "      \"schema\": \"CREATE TABLE dn_arpu_detail_sms_m1 (\\n  start_date timestamp,\\n  end_date timestamp,\\n  dn string,\\n  contract_id string,\\n  party_customer string,\\n  party_account string,\\n  product_id string,\\n  subprofile string,\\n  product_name string,\\n  segment string,\\n  gamme string,\\n  marche string,\\n  billing_type string,\\n  termination_type string,\\n  destination_type string,\\n  other_operator string,\\n  country string,\\n  call_direction string,\\n  balance_id string,\\n  balance_description string,\\n  balance_revenue_flag string,\\n  balance_unit string,\\n  balance_amount bigint,\\n  revenue_amount double,\\n  revenue_amount_ht double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.8869612216949463,\n",
            "        \"bm25_score\": 0.4896395031963375,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_arpu_detail_voice_m1\",\n",
            "      \"score\": 0.5828018409859048,\n",
            "      \"description\": \" Contient des informations d\\u00e9taill\\u00e9es sur les revenus des appels vocaux et les soldes pour chaque client ou compte par mois.\",\n",
            "      \"schema\": \"CREATE TABLE dn_arpu_detail_voice_m1 (\\n  start_date timestamp,\\n  end_date timestamp,\\n  dn string,\\n  party_customer string,\\n  party_account string,\\n  contract_id string,\\n  profile string,\\n  subprofile string,\\n  product_name string,\\n  segment string,\\n  gamme string,\\n  marche string,\\n  billing_type string,\\n  termination_type string,\\n  network_type string,\\n  destination_type string,\\n  other_operator string,\\n  country string,\\n  call_direction string,\\n  balance_id string,\\n  balance_description string,\\n  balance_revenue_flag string,\\n  balance_unit string,\\n  balance_amount bigint,\\n  revenue_amount double,\\n  revenue_amount_ht double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.8759679794311523,\n",
            "        \"bm25_score\": 0.4827261709010957,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_ln_prep_inter_sap_exp\",\n",
            "      \"score\": 0.5255129458740971,\n",
            "      \"description\": \" Suit les interactions SAP pour les clients pr\\u00e9pay\\u00e9s, incluant le num\\u00e9ro de s\\u00e9rie, la date de livraison et le code RC.\",\n",
            "      \"schema\": \"CREATE TABLE dn_ln_prep_inter_sap_exp (\\n  sernr string,\\n  date_liv string,\\n  rc bigint\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 1.0,\n",
            "        \"bm25_score\": 0.08504315291365723,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_ln_cpe_d1\",\n",
            "      \"score\": 0.5234599077755948,\n",
            "      \"description\": \" Contient des donn\\u00e9es sur les \\u00e9quipements clients (CPE) pour les utilisateurs professionnels, avec un focus sur l\\u2019utilisation et l\\u2019activation des \\u00e9quipements.\",\n",
            "      \"schema\": \"CREATE TABLE dn_ln_cpe_d1 (\\n  mdn string,\\n  date_activation timestamp,\\n  id_profil string,\\n  ci string,\\n  source string,\\n  ingestion_date timestamp\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9989823698997498,\n",
            "        \"bm25_score\": 0.07989574275239963,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"post_arpu_clb_d1\",\n",
            "      \"score\": 0.5099632358363269,\n",
            "      \"description\": \" Suivi quotidien du chiffre d'affaires g\\u00e9n\\u00e9r\\u00e9 par les tickets Clearbalance pour les clients postpay\\u00e9s.\",\n",
            "      \"schema\": \"CREATE TABLE post_arpu_clb_d1 (\\n  dn string,\\n  profile string,\\n  subprofile string,\\n  product_name string,\\n  gamme string,\\n  marche string,\\n  contract_id string,\\n  party_customer string,\\n  party_account string,\\n  clr_revenue double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9715709090232849,\n",
            "        \"bm25_score\": 0.08059260441561453,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_ln_cpr_d1\",\n",
            "      \"score\": 0.5097151399866223,\n",
            "      \"description\": \" Contient des donn\\u00e9es sur les r\\u00e9parations pour les \\u00e9quipements clients professionnels, y compris les activit\\u00e9s de maintenance.\",\n",
            "      \"schema\": \"CREATE TABLE dn_ln_cpr_d1 (\\n  mdn string,\\n  dat_actv_cpe timestamp,\\n  source string,\\n  date_cpr1 timestamp,\\n  date_cpr2 timestamp,\\n  mnt_cpr1 double,\\n  mnt_cpr2 double,\\n  ingestion_date timestamp\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.970649242401123,\n",
            "        \"bm25_score\": 0.08130172928686923,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"customer_d1\",\n",
            "      \"score\": 0.507701971724416,\n",
            "      \"description\": \" Stocke des informations quotidiennes sur les clients, y compris leurs d\\u00e9tails personnels et les informations de compte.\",\n",
            "      \"schema\": \"CREATE TABLE customer_d1 (\\n  customer_id string,\\n  billing_account_id string,\\n  creation_date timestamp,\\n  customer_type string,\\n  cin string,\\n  city string,\\n  profession string,\\n  address string,\\n  email_address string,\\n  ingestion_date string,\\n  id_date string\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9684808254241943,\n",
            "        \"bm25_score\": 0.07820519670772938,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"seg_rch_kpi_d1\",\n",
            "      \"score\": 0.5062524013105401,\n",
            "      \"description\": \" Mesure les indicateurs cl\\u00e9s de performance pour diff\\u00e9rents segments de clients, y compris le nombre de clients et les montants des ventes. Elle \\u00e9value la performance des segments de clients et identifie les segments les plus rentables.\",\n",
            "      \"schema\": \"CREATE TABLE seg_rch_kpi_d1 (\\n  start_date timestamp,\\n  num_total bigint,\\n  amnt_total_ttc double,\\n  top_rch_type string,\\n  top_ticket string,\\n  top_channel string,\\n  product_id string,\\n  subprofile string,\\n  product_name string,\\n  gamme string,\\n  marche string,\\n  segment string,\\n  billing_type string\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.947410523891449,\n",
            "        \"bm25_score\": 0.10849046454938532,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"customer_m1\",\n",
            "      \"score\": 0.5052032258061421,\n",
            "      \"description\": \" Stocke des informations mensuelles sur les clients, y compris leurs d\\u00e9tails personnels et les informations de compte.\",\n",
            "      \"schema\": \"CREATE TABLE customer_m1 (\\n  customer_id string,\\n  billing_account_id string,\\n  creation_date timestamp,\\n  customer_type string,\\n  cin string,\\n  city string,\\n  profession string,\\n  address string,\\n  email_address string,\\n  snapshot_date timestamp,\\n  ingestion_date string\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9634833335876465,\n",
            "        \"bm25_score\": 0.07820519670772938,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"post_arpu_recharge_m1\",\n",
            "      \"score\": 0.4973063574252834,\n",
            "      \"description\": \" Suit quels clients ont achet\\u00e9/utilis\\u00e9 quel produit/service, \\u00e0 quel moment, et combien ils ont \\u00e9t\\u00e9 factur\\u00e9s. L'objectif principal est de suivre toutes les activit\\u00e9s de facturation li\\u00e9es aux clients et \\u00e0 leurs contrats/produits pour des fins op\\u00e9rationnelles, financi\\u00e8res et de reporting.\",\n",
            "      \"schema\": \"CREATE TABLE post_arpu_recharge_m1 (\\n  start_date timestamp,\\n  end_date timestamp,\\n  party_account string,\\n  party_customer string,\\n  contract_id string,\\n  profile string,\\n  subprofile string,\\n  product_name string,\\n  gamme string,\\n  marche string,\\n  dn string,\\n  billed_recharge_amount double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9377740621566772,\n",
            "        \"bm25_score\": 0.09473108782314929,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"ref_region_province\",\n",
            "      \"score\": 0.49721675954011024,\n",
            "      \"description\": \" Contient des donn\\u00e9es de r\\u00e9f\\u00e9rence sur les r\\u00e9gions et les provinces, facilitant la cat\\u00e9gorisation et l'analyse des performances g\\u00e9ographiques. Elle aide \\u00e0 cibler les bons clients dans les bonnes r\\u00e9gions et \\u00e0 optimiser les strat\\u00e9gies marketing, de distribution et de vente.\",\n",
            "      \"schema\": \"CREATE TABLE ref_region_province (\\n  region string,\\n  province string\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9473109841346741,\n",
            "        \"bm25_score\": 0.0785375582425774,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"prep_arpu_clb_d1\",\n",
            "      \"score\": 0.4968656003475189,\n",
            "      \"description\": \" Affiche le profil client, les produits/services utilis\\u00e9s et le revenu CLR (revenu g\\u00e9n\\u00e9r\\u00e9 par chaque client). Elle mesure les revenus g\\u00e9n\\u00e9r\\u00e9s par chaque client en fonction de leur profil, de leur utilisation des produits et des d\\u00e9tails de contrat.\",\n",
            "      \"schema\": \"CREATE TABLE prep_arpu_clb_d1 (\\n  dn string,\\n  profile string,\\n  subprofile string,\\n  product_name string,\\n  gamme string,\\n  marche string,\\n  contract_id string,\\n  party_customer string,\\n  party_account string,\\n  clr_revenue double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9937312006950378,\n",
            "        \"bm25_score\": 0.0,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"post_arpu_interco_d1\",\n",
            "      \"score\": 0.4950204070558239,\n",
            "      \"description\": \" Suivi quotidien des revenus g\\u00e9n\\u00e9r\\u00e9s par les frais d\\u2019interconnexion pour les clients postpay\\u00e9s (ex  installation Wi-Fi), avec des informations sur le client et le service d'interconnexion.\",\n",
            "      \"schema\": \"CREATE TABLE post_arpu_interco_d1 (\\n  dn string,\\n  profile string,\\n  subprofile string,\\n  product_name string,\\n  gamme string,\\n  marche string,\\n  contract_id string,\\n  party_customer string,\\n  party_account string,\\n  traffic_type string,\\n  destination_type string,\\n  interco_revenue double\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9460933804512024,\n",
            "        \"bm25_score\": 0.07324572276740908,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_clr_blnce_kpi_d1\",\n",
            "      \"score\": 0.49268148695323866,\n",
            "      \"description\": \" Indicateurs de performance quotidiens li\\u00e9s aux soldes nets des clients.\",\n",
            "      \"schema\": \"CREATE TABLE dn_clr_blnce_kpi_d1 (\\n  start_date timestamp,\\n  dn string -- Directory Number,\\n  profile string -- Product Profile,\\n  subprofile string -- Product Subprofile,\\n  balance_id string -- Bucket Id,\\n  balance_bucket_description string,\\n  balance_amount double,\\n  num_events bigint -- Number of event,\\n  revenue_ht double -- Clear Balance Revenue,\\n  product_name string,\\n  gamme string,\\n  marche string,\\n  segment string,\\n  billing_type string,\\n  contract_id string,\\n  party_account string,\\n  party_customer string\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9437259435653687,\n",
            "        \"bm25_score\": 0.06939505056851447,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    },\n",
            "    {\n",
            "      \"table\": \"dn_ln_ident_d1\",\n",
            "      \"score\": 0.49170130555929664,\n",
            "      \"description\": \" Se concentre sur les donn\\u00e9es d'identification des clients professionnels, y compris l'identit\\u00e9 des drapeaux et l'activation des distributeurs.\",\n",
            "      \"schema\": \"CREATE TABLE dn_ln_ident_d1 (\\n  mdn string,\\n  date_souscription timestamp,\\n  identification_date timestamp,\\n  dealer_activation string,\\n  source string,\\n  flag_ident string,\\n  ingestion_date timestamp\\n);\",\n",
            "      \"details\": {\n",
            "        \"semantic_score\": 0.9341885447502136,\n",
            "        \"bm25_score\": 0.08202344394729938,\n",
            "        \"exact_match_score\": 0.0\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "## instructions\n",
            "Identify the most relevant table(s) required to answer the `User Request`: [\" je vous prie de nous fournir l'extraction MDN_par_PROFIL des clients iDar pour le Mois de mars  2025.\"]\n",
            "**Current Date for Reference:** 2025-04-27 (DD/MM/YYYY)\n",
            "\n",
            "\n",
            "###\n",
            "Determine the specific columns needed from these tables. **Strictly avoid `SELECT *`**. Only select the columns explicitly or implicitly asked for, or required for joins, filtering, or aggregation.\n",
            "\n",
            "- Identify the fields you will need in your SELECT clause based on the tables you have identified in previous steps\n",
            "- Identify constraints:\n",
            "            - Identify if a WHERE clause is needed\n",
            "              - If a WHERE clause is needed, identify constraints using the different SQL Operators like for example: AND, OR, IN, LIKE, ==,!=, <>..., or if a subquery is needed in which you will need to follow the same instructions mentioned here\n",
            "- **Domain-Aware Filtering (Smart Defaults)**: When the user request does not explicitly mention certain filter columns that are commonly used in telecom analytics(like crm_status ,..ect), do not include them blindly. These filters should only be added when they help reduce noise, narrow the population meaningfully, or align with standard telecom KPIs and best practices.\n",
            "- Make sure that you never return two columns with the same name, especially after joining two tables. You can differentiate the same column name by applying column_name + table_name.\n",
            "- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP, etc.) to the database.\n",
            "- Only use a table if at least one of its columns is required to answer the question. Do not include irrelevant tables.\n",
            "- Use only column names that are present in the schema for the selected table(s). Double-check that all columns used in SELECT, WHERE, JOIN, and GROUP BY exist in the schema.\n",
            "*Aggregation & Grouping (GROUP BY / HAVING):**\n",
            "    *   If the request involves aggregation (e.g., count, sum, average, max, min), use appropriate aggregate functions (`COUNT()`, `SUM()`, `AVG()`, `MAX()`, `MIN()`).\n",
            "    *   Include a `GROUP BY` clause listing all non-aggregated columns present in the `SELECT` clause.\n",
            "    *   Use a `HAVING` clause if filtering based on the *results* of aggregation is required.\n",
            "- You MUST double-check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n",
            "- If you cannot answer the question with the available database schema, return ‘I do not know.’\n",
            "\n",
            "**MANDATORY Date Filtering:**\n",
            "Date Format: Use the format 2025-04-27 depending on the context.\n",
            "If no date_type column exists, do not apply any date filter at all.\n",
            "Apply date filters only on columns with DATE, DATETIME, or TIMESTAMP types when the user mentions time (e.g. \"today\", \"last week\", \"in January\").\n",
            "Never filter on VARCHAR/TEXT/STRING columns, even if their name contains \"date\" Example(`id_date`,start_date, `id_date_ex` ect).\n",
            "If no time is mentioned, default to: WHERE date_col BETWEEN first day of current month AND (current date - 1 day).\n",
            "\n",
            "**End**\n",
            "\n",
            "## End of examples\n",
            "\n",
            "\n",
            "\n",
            "SQLQuery: Rephrase:\n",
            "\n",
            "\n",
            "\n",
            "📊 Résultat complet de Gemini (SQL Généré) :\n",
            "```sql\n",
            "SELECT\n",
            "  dn_parc_post_d1.mdn,\n",
            "  dn_parc_post_d1.crm_profil\n",
            "FROM\n",
            "  dn_parc_post_d1\n",
            "WHERE\n",
            "  dn_parc_post_d1.crm_profil = 'Idar'\n",
            "  AND dn_parc_post_d1.id_date BETWEEN '2025-03-01' AND '2025-03-31';\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Step 1: Load the selected schema from a JSON file\n",
        "with open('relevant_tables_results.json', 'r', encoding='utf-8') as f:\n",
        "    selected_schema = json.load(f)\n",
        "\n",
        "test_queries = [\n",
        "       \" je vous prie de nous fournir l'extraction MDN_par_PROFIL des clients iDar pour le Mois de mars  2025.\"]\n",
        "current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "# --- 7. SQL query generation prompt ---\n",
        "expert_prompt = f\"\"\"\n",
        "\n",
        "\n",
        "You are an expert-level SQL query generator with deep understanding of the Telecommunications (Telecom) domain and common analytics requests within this industry.\n",
        "\n",
        "# TASK\n",
        "\n",
        "Your primary task is to translate a user's natural language request into an accurate, efficient, and syntactically correct SQL query, based on the provided database schema.\n",
        "{json.dumps(selected_schema, indent=2)}\n",
        "\n",
        "## instructions\n",
        "Identify the most relevant table(s) required to answer the `User Request`: {test_queries}\n",
        "**Current Date for Reference:** {current_date} (DD/MM/YYYY)\n",
        "\n",
        "\n",
        "###\n",
        "Determine the specific columns needed from these tables. **Strictly avoid `SELECT *`**. Only select the columns explicitly or implicitly asked for, or required for joins, filtering, or aggregation.\n",
        "\n",
        "- Identify the fields you will need in your SELECT clause based on the tables you have identified in previous steps\n",
        "- Identify constraints:\n",
        "            - Identify if a WHERE clause is needed\n",
        "              - If a WHERE clause is needed, identify constraints using the different SQL Operators like for example: AND, OR, IN, LIKE, ==,!=, <>..., or if a subquery is needed in which you will need to follow the same instructions mentioned here\n",
        "- **Domain-Aware Filtering (Smart Defaults)**: When the user request does not explicitly mention certain filter columns that are commonly used in telecom analytics(like crm_status ,..ect), do not include them blindly. These filters should only be added when they help reduce noise, narrow the population meaningfully, or align with standard telecom KPIs and best practices.\n",
        "- Make sure that you never return two columns with the same name, especially after joining two tables. You can differentiate the same column name by applying column_name + table_name.\n",
        "- DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP, etc.) to the database.\n",
        "- Only use a table if at least one of its columns is required to answer the question. Do not include irrelevant tables.\n",
        "- Use only column names that are present in the schema for the selected table(s). Double-check that all columns used in SELECT, WHERE, JOIN, and GROUP BY exist in the schema.\n",
        "*Aggregation & Grouping (GROUP BY / HAVING):**\n",
        "    *   If the request involves aggregation (e.g., count, sum, average, max, min), use appropriate aggregate functions (`COUNT()`, `SUM()`, `AVG()`, `MAX()`, `MIN()`).\n",
        "    *   Include a `GROUP BY` clause listing all non-aggregated columns present in the `SELECT` clause.\n",
        "    *   Use a `HAVING` clause if filtering based on the *results* of aggregation is required.\n",
        "- You MUST double-check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n",
        "- If you cannot answer the question with the available database schema, return ‘I do not know.’\n",
        "\n",
        "**MANDATORY Date Filtering:**\n",
        "Date Format: Use the format {current_date} depending on the context.\n",
        "If no date_type column exists, do not apply any date filter at all.\n",
        "Apply date filters only on columns with DATE, DATETIME, or TIMESTAMP types when the user mentions time (e.g. \"today\", \"last week\", \"in January\").\n",
        "Never filter on VARCHAR/TEXT/STRING columns, even if their name contains \"date\" Example(`id_date`,start_date, `id_date_ex` ect).\n",
        "If no time is mentioned, default to: WHERE date_col BETWEEN first day of current month AND (current date - 1 day).\n",
        "\n",
        "**End**\n",
        "\n",
        "## End of examples\n",
        "\n",
        "\n",
        "\n",
        "SQLQuery: Rephrase:\n",
        "\n",
        "\"\"\"\n",
        "# Pass the 'expert_prompt' variable containing the prompt string instead of the file object 'f'\n",
        "response = model.generate_content(expert_prompt)\n",
        "print(expert_prompt)\n",
        "# --- 9. Display response ---\n",
        "print(\"\\n📊 Résultat complet de Gemini (SQL Généré) :\")\n",
        "print(response.text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDcGcZ-VbRgg"
      },
      "source": [
        "**Interrogation de Gemini pour générer la requête SQL :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMiHco6gbdUJ"
      },
      "source": [
        "**SQL générée**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "67TAUZSemuiM",
        "outputId": "87cdaa22-d19c-4684-a491-fc25c36e20d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'metadata.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-59f6b170da7c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 4. Load metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metadata.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'metadata.json'"
          ]
        }
      ],
      "source": [
        "# 1. Install necessary package\n",
        "!pip install -q google-generativeai\n",
        "\n",
        "# 2. Imports\n",
        "import json\n",
        "import google.generativeai as genai\n",
        "from collections import defaultdict\n",
        "\n",
        "# 3. Configure Gemini API\n",
        "genai.configure(api_key=\"AIzaSyBdLqxG6_hcuer7C2IVRrbfxdYTUa20nSc\")\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "\n",
        "# 4. Load metadata\n",
        "with open(\"metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# 5. Organize schema\n",
        "table_descriptions = defaultdict(list)\n",
        "for entry in metadata:\n",
        "    tbl_name = entry.get(\"tbl_name\", \"\")\n",
        "    column_name = entry.get(\"column_name\", \"\")\n",
        "    desc_column = entry.get(\"desc_column\", \"\")\n",
        "    exp_column = entry.get(\"exp_column\", \"\")\n",
        "\n",
        "    column_desc = f\"{column_name}\"\n",
        "    if desc_column:\n",
        "        column_desc += f\" - {desc_column}\"\n",
        "    if exp_column:\n",
        "        column_desc += f\" ({exp_column})\"\n",
        "\n",
        "    table_descriptions[tbl_name].append(column_desc)\n",
        "\n",
        "tables = {tbl: \". \".join(cols) for tbl, cols in table_descriptions.items()}\n",
        "\n",
        "# 6. Set user request and date\n",
        "user_request = \"\"\"Bonjour \\nJ'espère que vous allez bien. \\nPourriez-vous, s'il vous plaît, nous fournir une extraction des données contenant les informations suivantes pour chauque jour fournir un détail de nombre mdn cpe \\nMerci d'avance pour votre aide. N'hésitez pas à me contacter si vous avez besoin de précisions. \\nBien cordialement,\"\"\"\n",
        "current_date = \"19/04/2025\"  # format DD/MM/YYYY\n",
        "\n",
        "# 7. Format the input using your SQL expert prompt\n",
        "expert_prompt = f\"\"\"\n",
        "# ROLE: Expert SQL Query Generator (Telecom Analytics)\n",
        "\n",
        "You are an expert-level SQL query generator with deep understanding of the Telecommunications (Telecom) domain and common analytics requests within this industry.\n",
        "\n",
        "# TASK\n",
        "Your primary task is to translate a user's natural language request into an accurate, efficient, and syntactically correct SQL query, based on the provided database schema.\n",
        "\n",
        "# INPUTS\n",
        "1. User Request: {user_request}\n",
        "2. Database Schema: {json.dumps(tables, indent=2)}\n",
        "3. Current Date: {current_date}\n",
        "\n",
        "# OUTPUT EXPECTED\n",
        "- If the request can be fulfilled using the schema: A single, executable SQL query.\n",
        "- If not: Respond with \"I cannot answer this request with the provided schema.\"\n",
        "\n",
        "# RULES\n",
        "- Use only provided table and column names EXACTLY as in schema.\n",
        "- Never use SELECT *.\n",
        "- Always include date filtering using the most relevant column and the provided current date ({current_date}).\n",
        "- Ensure joins, filters, and logic align with the user request.\n",
        "\n",
        "# OUTPUT FORMAT\n",
        "Question: {user_request}\n",
        "SQLQuery:\n",
        "-- your SQL query here\n",
        "SQLResult: -- result (can be descriptive)\n",
        "Answer: -- final interpretation if needed\n",
        "\"\"\"\n",
        "\n",
        "# 8. Query Gemini\n",
        "response = model.generate_content(expert_prompt)\n",
        "\n",
        "# 9. Display response\n",
        "print(\"\\n📊 Résultat complet de Gemini (SQL Généré) :\")\n",
        "print(response.text.strip())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPBVpZLtBhX67fWxzeCzhGw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}